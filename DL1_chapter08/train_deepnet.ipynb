{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "italic-glass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.235649860628482\n",
      "=== epoch:1, train acc:0.122, test acc:0.125 ===\n",
      "train loss:2.291742838456386\n",
      "train loss:2.2493139962279782\n",
      "train loss:2.251360831023519\n",
      "train loss:2.2922323563470197\n",
      "train loss:2.2714857580453653\n",
      "train loss:2.2488947747769936\n",
      "train loss:2.251384740337249\n",
      "train loss:2.229606241847097\n",
      "train loss:2.1902334220209485\n",
      "train loss:2.221262805929476\n",
      "train loss:2.202371188125621\n",
      "train loss:2.207784130832141\n",
      "train loss:2.1271614997406205\n",
      "train loss:2.1484223120077446\n",
      "train loss:2.1021589433290457\n",
      "train loss:2.0516427882603163\n",
      "train loss:2.1801521233750027\n",
      "train loss:1.9640837280948826\n",
      "train loss:2.0356923767966286\n",
      "train loss:2.0639197995242373\n",
      "train loss:1.9850021575940258\n",
      "train loss:2.067518357107423\n",
      "train loss:1.9028364512777276\n",
      "train loss:1.9828008569983209\n",
      "train loss:1.9978088271398684\n",
      "train loss:1.9570326079072806\n",
      "train loss:1.8717497683104525\n",
      "train loss:1.9491034914500798\n",
      "train loss:1.88371267380611\n",
      "train loss:1.9609754138584685\n",
      "train loss:1.9162159029766748\n",
      "train loss:1.9052065424261417\n",
      "train loss:1.9498969040119973\n",
      "train loss:1.8010824907992729\n",
      "train loss:1.7927219962709353\n",
      "train loss:1.9238014700501467\n",
      "train loss:1.7611421448045437\n",
      "train loss:1.7808632379918832\n",
      "train loss:1.5725044768015926\n",
      "train loss:1.7478146164036117\n",
      "train loss:1.7736908862884484\n",
      "train loss:1.8962131090507757\n",
      "train loss:1.9128532479338523\n",
      "train loss:1.8544684957310862\n",
      "train loss:1.7494508523188248\n",
      "train loss:1.6994816539480695\n",
      "train loss:1.6996963516021961\n",
      "train loss:1.8626102067359809\n",
      "train loss:1.6626234204077692\n",
      "train loss:1.823293330626141\n",
      "train loss:1.5467858919882165\n",
      "train loss:1.752000009156232\n",
      "train loss:1.7990239248141429\n",
      "train loss:1.6299521225699665\n",
      "train loss:1.7624189332305624\n",
      "train loss:1.654458422019098\n",
      "train loss:1.68615011981523\n",
      "train loss:1.6505559474857971\n",
      "train loss:1.5220368379701785\n",
      "train loss:1.5772478744199685\n",
      "train loss:1.6716442516385828\n",
      "train loss:1.6705917189230564\n",
      "train loss:1.6220329642401112\n",
      "train loss:1.7101395651831583\n",
      "train loss:1.6936899218028314\n",
      "train loss:1.624878567598948\n",
      "train loss:1.7552829651871362\n",
      "train loss:1.5687436282850329\n",
      "train loss:1.4043327851473268\n",
      "train loss:1.4063089506853796\n",
      "train loss:1.581815190594628\n",
      "train loss:1.612628390468691\n",
      "train loss:1.5563485107732853\n",
      "train loss:1.4305624976379958\n",
      "train loss:1.5298560779145496\n",
      "train loss:1.5912863898760095\n",
      "train loss:1.4914867252315276\n",
      "train loss:1.3873720710566488\n",
      "train loss:1.618845266676594\n",
      "train loss:1.4838357213309505\n",
      "train loss:1.660110122584001\n",
      "train loss:1.5062303721948889\n",
      "train loss:1.4911277476841456\n",
      "train loss:1.7325314644435292\n",
      "train loss:1.5197926559468475\n",
      "train loss:1.5569246886073393\n",
      "train loss:1.6862781959525597\n",
      "train loss:1.4495340081064838\n",
      "train loss:1.423674653171647\n",
      "train loss:1.469721424362916\n",
      "train loss:1.3905176160677877\n",
      "train loss:1.4155394186035009\n",
      "train loss:1.3767276942186526\n",
      "train loss:1.4125565866809802\n",
      "train loss:1.6308062900564209\n",
      "train loss:1.521206261816117\n",
      "train loss:1.4660969804852961\n",
      "train loss:1.6059119213620048\n",
      "train loss:1.2380203964670262\n",
      "train loss:1.5026115019386992\n",
      "train loss:1.376135424174454\n",
      "train loss:1.4198366880187252\n",
      "train loss:1.5547529184568634\n",
      "train loss:1.6424080014957005\n",
      "train loss:1.5156963447117298\n",
      "train loss:1.4043574445167732\n",
      "train loss:1.5616023906735288\n",
      "train loss:1.3220785284192333\n",
      "train loss:1.226325089398075\n",
      "train loss:1.4626404906873471\n",
      "train loss:1.5082916310043615\n",
      "train loss:1.50433102584236\n",
      "train loss:1.3890346792336792\n",
      "train loss:1.4795084230325393\n",
      "train loss:1.2279580526783795\n",
      "train loss:1.4631894627350395\n",
      "train loss:1.4998749157242488\n",
      "train loss:1.307233274984993\n",
      "train loss:1.4269488825545737\n",
      "train loss:1.4471524547354602\n",
      "train loss:1.5322115377932946\n",
      "train loss:1.622140232919854\n",
      "train loss:1.5029170392069446\n",
      "train loss:1.2105575574005163\n",
      "train loss:1.4537719130388063\n",
      "train loss:1.5081562947955123\n",
      "train loss:1.4386137124881788\n",
      "train loss:1.4048935374306364\n",
      "train loss:1.4232058704488995\n",
      "train loss:1.3448948642189442\n",
      "train loss:1.5490322375301095\n",
      "train loss:1.4772500977704561\n",
      "train loss:1.2507339818529992\n",
      "train loss:1.5784510450415343\n",
      "train loss:1.5056981749959082\n",
      "train loss:1.4257567102017454\n",
      "train loss:1.2229170742109854\n",
      "train loss:0.9973470848778304\n",
      "train loss:1.3485054404235757\n",
      "train loss:1.2042944032411398\n",
      "train loss:1.3687728179519487\n",
      "train loss:1.399244053616509\n",
      "train loss:1.4989777463117846\n",
      "train loss:1.201110740955819\n",
      "train loss:1.3629412125833318\n",
      "train loss:1.354884315748101\n",
      "train loss:1.5429700423389603\n",
      "train loss:1.4505624050323758\n",
      "train loss:1.2775983616526834\n",
      "train loss:1.3054403228300773\n",
      "train loss:1.284402541781066\n",
      "train loss:1.3507078117519058\n",
      "train loss:1.425158768615228\n",
      "train loss:1.2286640185339548\n",
      "train loss:1.3702040840033456\n",
      "train loss:1.3522789347374848\n",
      "train loss:1.274402756348816\n",
      "train loss:1.6517279558797944\n",
      "train loss:1.3866632550264981\n",
      "train loss:1.4600774860794237\n",
      "train loss:1.4346463175137614\n",
      "train loss:1.1883996694688117\n",
      "train loss:1.5437914299814801\n",
      "train loss:1.207958765789764\n",
      "train loss:1.3105045676006517\n",
      "train loss:1.288968620516803\n",
      "train loss:1.352453816861742\n",
      "train loss:1.3625798838355399\n",
      "train loss:1.4508919635522168\n",
      "train loss:1.253431402084886\n",
      "train loss:1.2350383159439111\n",
      "train loss:1.2960556174667324\n",
      "train loss:1.5022627200302694\n",
      "train loss:1.3286947921833017\n",
      "train loss:1.2996864996986093\n",
      "train loss:1.402989072731009\n",
      "train loss:1.4603971637146949\n",
      "train loss:1.1374248956208692\n",
      "train loss:1.2776075337977817\n",
      "train loss:1.4030501421905266\n",
      "train loss:1.2156359753717392\n",
      "train loss:1.2196336924216151\n",
      "train loss:1.4459466469158861\n",
      "train loss:1.2168681954601022\n",
      "train loss:1.4594887489600017\n",
      "train loss:1.1701259906410417\n",
      "train loss:1.3493597903614154\n",
      "train loss:1.1029646312386756\n",
      "train loss:1.2746043553344886\n",
      "train loss:1.2959075223398477\n",
      "train loss:1.470712934535168\n",
      "train loss:1.3449772697579658\n",
      "train loss:1.2736922250949367\n",
      "train loss:1.3678871748838843\n",
      "train loss:1.3738022044241582\n",
      "train loss:1.2309229563697626\n",
      "train loss:1.3312118660725516\n",
      "train loss:1.3580657952941573\n",
      "train loss:1.2750399349218358\n",
      "train loss:1.2713754042490326\n",
      "train loss:1.208369445486372\n",
      "train loss:1.5020879547526422\n",
      "train loss:1.2941229778309107\n",
      "train loss:1.281868079308776\n",
      "train loss:1.5264867452779505\n",
      "train loss:1.3732537354177217\n",
      "train loss:1.2424908276815356\n",
      "train loss:1.2521476881565805\n",
      "train loss:1.1849801905571191\n",
      "train loss:1.2204826906984816\n",
      "train loss:1.392605204622603\n",
      "train loss:1.2167862814028445\n",
      "train loss:1.2703743204408005\n",
      "train loss:1.4024883327137292\n",
      "train loss:1.498500898252571\n",
      "train loss:1.2500493516576199\n",
      "train loss:1.252498542598408\n",
      "train loss:1.232002435048972\n",
      "train loss:1.209232951851367\n",
      "train loss:1.0776509430359063\n",
      "train loss:1.332614612837926\n",
      "train loss:1.3609632362916342\n",
      "train loss:1.2073982332020783\n",
      "train loss:1.4180282443180783\n",
      "train loss:1.3345012781183028\n",
      "train loss:1.327459783223182\n",
      "train loss:1.2337881067580188\n",
      "train loss:1.0118445192663499\n",
      "train loss:1.2741292701855114\n",
      "train loss:1.3948000510518963\n",
      "train loss:1.3369133273728735\n",
      "train loss:1.0685717068195553\n",
      "train loss:1.5138942077465143\n",
      "train loss:1.3655074626823873\n",
      "train loss:1.2194396812892139\n",
      "train loss:1.1891517576904505\n",
      "train loss:1.4826637732856114\n",
      "train loss:1.4213924384622745\n",
      "train loss:1.4092998753565353\n",
      "train loss:1.2860953646513371\n",
      "train loss:1.2755770389890222\n",
      "train loss:1.2134802485590686\n",
      "train loss:1.3602187740680673\n",
      "train loss:1.265599500997678\n",
      "train loss:1.1857488599002743\n",
      "train loss:1.0766241174906512\n",
      "train loss:1.2397069592771215\n",
      "train loss:1.1644283877592088\n",
      "train loss:1.244610907172734\n",
      "train loss:1.384210902698567\n",
      "train loss:1.277821780553383\n",
      "train loss:1.3135926095397499\n",
      "train loss:1.0386329240824554\n",
      "train loss:1.275091519497816\n",
      "train loss:1.2886205333694247\n",
      "train loss:1.308975170216425\n",
      "train loss:1.0785549024027201\n",
      "train loss:1.3255119212700688\n",
      "train loss:1.2726508084325665\n",
      "train loss:1.2318800774096854\n",
      "train loss:1.1737804188384606\n",
      "train loss:1.0737956401094992\n",
      "train loss:1.1958485965786425\n",
      "train loss:1.0559880539510702\n",
      "train loss:1.3055450649700284\n",
      "train loss:1.385510694446172\n",
      "train loss:1.2811801735378145\n",
      "train loss:1.294709765257303\n",
      "train loss:1.3174892351964462\n",
      "train loss:1.2859418336092208\n",
      "train loss:1.1032847793600766\n",
      "train loss:1.2357528945569984\n",
      "train loss:1.2178581048489534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.1584414525545308\n",
      "train loss:1.269662099674652\n",
      "train loss:1.1360423654142673\n",
      "train loss:1.0625973401818007\n",
      "train loss:1.0815489602577215\n",
      "train loss:1.297429768789998\n",
      "train loss:1.248217967734418\n",
      "train loss:1.262461606385718\n",
      "train loss:1.2999411240464689\n",
      "train loss:1.2226700381362237\n",
      "train loss:1.2858235496388035\n",
      "train loss:1.2803522998374184\n",
      "train loss:1.1489095270327179\n",
      "train loss:0.9251523865222419\n",
      "train loss:1.1801925321661864\n",
      "train loss:1.104688809534617\n",
      "train loss:1.0969025041206033\n",
      "train loss:1.2892849996928615\n",
      "train loss:1.1418306426650102\n",
      "train loss:1.1931995374622126\n",
      "train loss:1.109767289768298\n",
      "train loss:1.3485666071597036\n",
      "train loss:1.0731309045041006\n",
      "train loss:1.2123898716695023\n",
      "train loss:1.2009386385234364\n",
      "train loss:1.1182003659862503\n",
      "train loss:1.1399105493396375\n",
      "train loss:1.1280798300306127\n",
      "train loss:1.215550036375934\n",
      "train loss:1.234989787652841\n",
      "train loss:1.2628833553832144\n",
      "train loss:0.9388704644254596\n",
      "train loss:1.139868865383149\n",
      "train loss:1.276886503068616\n",
      "train loss:1.2467702375656442\n",
      "train loss:1.0846917815681063\n",
      "train loss:1.022879342309393\n",
      "train loss:1.2413539523953334\n",
      "train loss:1.209173759717373\n",
      "train loss:1.1912383123410333\n",
      "train loss:1.1315230057902403\n",
      "train loss:1.0048403982402798\n",
      "train loss:1.3427998164591914\n",
      "train loss:1.2613046488122521\n",
      "train loss:0.9019109865131625\n",
      "train loss:1.1490531820454222\n",
      "train loss:1.1005274353696182\n",
      "train loss:1.3127578594041975\n",
      "train loss:1.0164869787235449\n",
      "train loss:1.2645867108588666\n",
      "train loss:1.2693646873500093\n",
      "train loss:1.1268094523981675\n",
      "train loss:1.1575351481117793\n",
      "train loss:1.0420522741127456\n",
      "train loss:1.2358393963705279\n",
      "train loss:1.2701663194642483\n",
      "train loss:1.2124655564225393\n",
      "train loss:1.1602768306728115\n",
      "train loss:1.262150180756477\n",
      "train loss:1.1631989215224388\n",
      "train loss:0.9897848835632973\n",
      "train loss:1.2057519584035001\n",
      "train loss:1.143292588801886\n",
      "train loss:1.1580592224567885\n",
      "train loss:1.4306896111377512\n",
      "train loss:0.9971063717982621\n",
      "train loss:1.0764071570994405\n",
      "train loss:1.2537031901795976\n",
      "train loss:1.349008635202338\n",
      "train loss:1.2176249276897733\n",
      "train loss:1.154521663316769\n",
      "train loss:1.3155659562325623\n",
      "train loss:1.2171215146190493\n",
      "train loss:1.1997061248691672\n",
      "train loss:1.3018979343669088\n",
      "train loss:1.129459245847589\n",
      "train loss:1.2348541624039209\n",
      "train loss:1.1997632073771196\n",
      "train loss:1.138992990860255\n",
      "train loss:1.0559798802220763\n",
      "train loss:1.0389913008714475\n",
      "train loss:1.1026777477828473\n",
      "train loss:1.224521650044749\n",
      "train loss:1.1904632183649875\n",
      "train loss:0.9967281732432207\n",
      "train loss:1.151261156042519\n",
      "train loss:1.1772376695125766\n",
      "train loss:1.285977019601337\n",
      "train loss:1.1193283107836676\n",
      "train loss:1.0828636004739856\n",
      "train loss:1.1974912923114376\n",
      "train loss:1.0983959542024468\n",
      "train loss:1.2066062240348716\n",
      "train loss:1.0458197257506956\n",
      "train loss:1.1918179605227477\n",
      "train loss:1.247878965120005\n",
      "train loss:1.0751480438460512\n",
      "train loss:1.2132829891341856\n",
      "train loss:1.0004110053512483\n",
      "train loss:1.1447438780768795\n",
      "train loss:1.220675294858126\n",
      "train loss:1.3866773056193418\n",
      "train loss:1.0303721739668976\n",
      "train loss:1.2516957720130422\n",
      "train loss:0.9792417722128026\n",
      "train loss:1.1034928584114554\n",
      "train loss:1.117069483458027\n",
      "train loss:1.0798926950346397\n",
      "train loss:1.0609303701790374\n",
      "train loss:1.176638673051413\n",
      "train loss:1.3277269118631665\n",
      "train loss:1.0321043326812318\n",
      "train loss:0.9948499167598218\n",
      "train loss:1.1384586166600914\n",
      "train loss:1.0913228682840928\n",
      "train loss:1.1721353949489448\n",
      "train loss:1.227040224805644\n",
      "train loss:1.1563485745356936\n",
      "train loss:1.2060103669532722\n",
      "train loss:0.9977730661191466\n",
      "train loss:1.2276151379147473\n",
      "train loss:1.0597537871413218\n",
      "train loss:1.2598629525034457\n",
      "train loss:1.1870736056009294\n",
      "train loss:1.3542897149747448\n",
      "train loss:1.1302279148433605\n",
      "train loss:1.1595873176634819\n",
      "train loss:1.084193793023128\n",
      "train loss:1.000739671363715\n",
      "train loss:1.1507606818811933\n",
      "train loss:1.1850891675786552\n",
      "train loss:1.1096710833558232\n",
      "train loss:1.3316391532405976\n",
      "train loss:1.1953547674554799\n",
      "train loss:1.0645615049249424\n",
      "train loss:1.0701913628948787\n",
      "train loss:1.2174839679599527\n",
      "train loss:1.046993161709381\n",
      "train loss:1.2037590682665753\n",
      "train loss:1.0801769908306356\n",
      "train loss:1.1547967846627787\n",
      "train loss:1.187803445214645\n",
      "train loss:1.0890627397562347\n",
      "train loss:1.129047917979687\n",
      "train loss:0.9700926007330548\n",
      "train loss:1.1502565256073296\n",
      "train loss:1.0519701539632365\n",
      "train loss:1.2208094536283298\n",
      "train loss:1.270723008217706\n",
      "train loss:1.020360593921981\n",
      "train loss:1.0298820281062713\n",
      "train loss:1.2382864918683603\n",
      "train loss:0.9151323082762288\n",
      "train loss:1.3599442909301425\n",
      "train loss:1.1454908636483927\n",
      "train loss:1.2362009858567997\n",
      "train loss:1.284040692247644\n",
      "train loss:1.3567539148688161\n",
      "train loss:1.0937276051317009\n",
      "train loss:1.0299765533471774\n",
      "train loss:1.1854239943298006\n",
      "train loss:1.165922664484032\n",
      "train loss:0.9655528678113131\n",
      "train loss:1.1710824866050202\n",
      "train loss:0.9967403910420373\n",
      "train loss:0.9604782735681673\n",
      "train loss:1.0134020020196306\n",
      "train loss:1.2456064090533883\n",
      "train loss:0.9915336275263792\n",
      "train loss:1.1603927017610731\n",
      "train loss:1.067670680458305\n",
      "train loss:1.1260198537975112\n",
      "train loss:1.0686635655442975\n",
      "train loss:1.0762643860284278\n",
      "train loss:1.0961266474325235\n",
      "train loss:1.2391700070578489\n",
      "train loss:1.1070125201884873\n",
      "train loss:1.0632294084712617\n",
      "train loss:0.9790354026218082\n",
      "train loss:1.1737772539061873\n",
      "train loss:1.3324237820509057\n",
      "train loss:0.9811718937790799\n",
      "train loss:1.0619134931139842\n",
      "train loss:1.1924265195407717\n",
      "train loss:1.235091843650082\n",
      "train loss:1.1978653715856002\n",
      "train loss:1.2037351992071617\n",
      "train loss:0.9838275918880562\n",
      "train loss:1.1132793201311086\n",
      "train loss:1.1256014813228763\n",
      "train loss:1.13507701992065\n",
      "train loss:1.1960254410872941\n",
      "train loss:1.2049650665358684\n",
      "train loss:0.9525650122050798\n",
      "train loss:1.1289866142354945\n",
      "train loss:1.0250639744370493\n",
      "train loss:1.0201762528771503\n",
      "train loss:0.8861250898546382\n",
      "train loss:1.106745760567727\n",
      "train loss:1.2274012593966004\n",
      "train loss:1.014647620588814\n",
      "train loss:1.089780349355343\n",
      "train loss:1.1454944765787665\n",
      "train loss:1.1324565734053147\n",
      "train loss:1.0709708713738662\n",
      "train loss:1.0819395876471314\n",
      "train loss:1.1280450098039032\n",
      "train loss:1.11982475784877\n",
      "train loss:1.0876849964141984\n",
      "train loss:1.2701184824619267\n",
      "train loss:1.0537577507867733\n",
      "train loss:0.981702268985858\n",
      "train loss:1.1117387441104913\n",
      "train loss:1.1823598055481275\n",
      "train loss:1.0971844617521884\n",
      "train loss:1.0697747637995771\n",
      "train loss:1.1434925955798985\n",
      "train loss:1.0283341682075267\n",
      "train loss:1.1191930696687753\n",
      "train loss:0.9891351947414939\n",
      "train loss:1.0580510099708456\n",
      "train loss:1.0104241540979577\n",
      "train loss:1.0212944305037082\n",
      "train loss:1.138859519191249\n",
      "train loss:0.9873304419146048\n",
      "train loss:1.141904177782323\n",
      "train loss:0.8655259243085435\n",
      "train loss:0.9791780700609459\n",
      "train loss:1.2056169278351276\n",
      "train loss:1.2184656769486417\n",
      "train loss:0.9704134571981348\n",
      "train loss:1.226606478994633\n",
      "train loss:1.1916699048588724\n",
      "train loss:1.0344342843986274\n",
      "train loss:1.0896748272120533\n",
      "train loss:1.163759239986401\n",
      "train loss:1.1062886174780187\n",
      "train loss:0.9852020845659867\n",
      "train loss:0.9673470039662445\n",
      "train loss:0.8687468508160784\n",
      "train loss:1.2016575851755276\n",
      "train loss:1.084627135431019\n",
      "train loss:1.06871036195996\n",
      "train loss:1.194772557568882\n",
      "train loss:1.116228865967282\n",
      "train loss:1.076851045982456\n",
      "train loss:1.2258104031139028\n",
      "train loss:1.2413864482281651\n",
      "train loss:1.0146132705890658\n",
      "train loss:0.9443302396395885\n",
      "train loss:1.0250462306115955\n",
      "train loss:1.0332667747117048\n",
      "train loss:1.0642979738891603\n",
      "train loss:1.1087736199324147\n",
      "train loss:1.1783030951694462\n",
      "train loss:1.2434340444280123\n",
      "train loss:1.0475334605314215\n",
      "train loss:1.0269104479404572\n",
      "train loss:0.9077906305414929\n",
      "train loss:1.076210921555723\n",
      "train loss:1.1505448993029237\n",
      "train loss:1.1132471696344173\n",
      "train loss:1.2382408111570036\n",
      "train loss:1.0169627150496598\n",
      "train loss:0.9362633713779567\n",
      "train loss:0.9142249125849006\n",
      "train loss:1.1548936677730874\n",
      "train loss:1.0589989445395218\n",
      "train loss:1.057342770358521\n",
      "train loss:1.1819402667018672\n",
      "train loss:1.0285308245976832\n",
      "train loss:0.9751136771594617\n",
      "train loss:0.9890628922982425\n",
      "train loss:1.1917033432089448\n",
      "train loss:1.0714853224439094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:1.0181023585294395\n",
      "train loss:1.0334604990010432\n",
      "train loss:1.1196992900031644\n",
      "train loss:1.0449204182120482\n",
      "train loss:1.2240195921608596\n",
      "train loss:1.1545209887325756\n",
      "train loss:1.0691486572071929\n",
      "train loss:0.9443672610816044\n",
      "train loss:1.1078608908128593\n",
      "train loss:1.1949535887013676\n",
      "train loss:1.0260236130217526\n",
      "train loss:1.1084929794266336\n",
      "train loss:1.063955918512929\n",
      "train loss:1.055161065567969\n",
      "train loss:1.19609916822159\n",
      "train loss:1.0641499583223843\n",
      "train loss:1.225562422604008\n",
      "train loss:1.040112490310493\n",
      "train loss:1.0164508830526358\n",
      "train loss:1.077775613995684\n",
      "train loss:1.0019667171592415\n",
      "train loss:1.096054092644711\n",
      "train loss:1.1206479595869507\n",
      "train loss:1.1527608425719722\n",
      "train loss:1.1011202823409985\n",
      "train loss:1.0827821719031734\n",
      "train loss:0.9633483097634749\n",
      "train loss:1.0810074338520557\n",
      "train loss:0.9637644114655367\n",
      "train loss:1.1415114220195464\n",
      "train loss:1.1047132666983805\n",
      "train loss:1.1409221309915436\n",
      "train loss:1.0720723831635923\n",
      "train loss:0.8927356970918784\n",
      "train loss:1.032977550007161\n",
      "train loss:1.1927271745201988\n",
      "train loss:1.0749373407165599\n",
      "train loss:1.3048354670424296\n",
      "train loss:1.093259720430161\n",
      "train loss:0.9756335805177989\n",
      "train loss:1.0115224026434089\n",
      "train loss:1.1317022598515514\n",
      "train loss:1.1478418530492305\n",
      "train loss:1.0110130385057001\n",
      "train loss:1.0730316490595522\n",
      "train loss:1.049995059874978\n",
      "train loss:0.9576071777234408\n",
      "train loss:0.9541444982369026\n",
      "train loss:1.0631900431534214\n",
      "train loss:1.1444131815862812\n",
      "train loss:1.1250098332387863\n",
      "train loss:1.1969574316500873\n",
      "=== epoch:2, train acc:0.978, test acc:0.982 ===\n",
      "train loss:0.8950744294030766\n",
      "train loss:1.1220010103543645\n",
      "train loss:1.1121155211403444\n",
      "train loss:1.1916286101121447\n",
      "train loss:1.101702206480237\n",
      "train loss:1.0920478883976008\n",
      "train loss:1.1333411208816668\n",
      "train loss:1.1590434619796564\n",
      "train loss:1.1544489560551152\n",
      "train loss:1.0814261263714078\n",
      "train loss:0.9199918908396308\n",
      "train loss:0.8867186041096009\n",
      "train loss:0.9146927798016827\n",
      "train loss:1.0148234017755184\n",
      "train loss:0.9680663906746493\n",
      "train loss:1.0024614412842652\n",
      "train loss:1.059105286768922\n",
      "train loss:1.172710026429861\n",
      "train loss:1.0573358127934602\n",
      "train loss:0.9992555463867654\n",
      "train loss:1.0433322963716953\n",
      "train loss:1.1001784120000047\n",
      "train loss:0.8334534559048444\n",
      "train loss:0.9160501536658465\n",
      "train loss:1.247014586685516\n",
      "train loss:1.0319189904849373\n",
      "train loss:1.065063533148736\n",
      "train loss:1.0791800766770692\n",
      "train loss:1.1386622724914235\n",
      "train loss:1.0807472554290378\n",
      "train loss:1.0158947995613727\n",
      "train loss:1.0915222717750919\n",
      "train loss:1.016967840471179\n",
      "train loss:1.0837060370657712\n",
      "train loss:1.0441686108300825\n",
      "train loss:1.1507329355162648\n",
      "train loss:0.8969121417477807\n",
      "train loss:1.1309496298005195\n",
      "train loss:1.1837008005840548\n",
      "train loss:1.211727145595466\n",
      "train loss:0.9537193847396872\n",
      "train loss:1.0712685012480367\n",
      "train loss:1.0267326923930458\n",
      "train loss:1.1786107464055655\n",
      "train loss:0.8955139417903664\n",
      "train loss:0.9220357747987638\n",
      "train loss:0.9607841409367956\n",
      "train loss:0.9026770817588343\n",
      "train loss:1.096612784442936\n",
      "train loss:1.0915742354284705\n",
      "train loss:1.0656014037173684\n",
      "train loss:0.9622219996553372\n",
      "train loss:1.0741259723077747\n",
      "train loss:1.1508770738435745\n",
      "train loss:1.068685133444027\n",
      "train loss:0.9953432031567376\n",
      "train loss:1.0585160965460922\n",
      "train loss:0.9895867939986283\n",
      "train loss:0.9162945739223852\n",
      "train loss:0.8831290677862822\n",
      "train loss:1.0177319033447827\n",
      "train loss:0.9940958437383501\n",
      "train loss:1.1204412330193012\n",
      "train loss:0.986683938224873\n",
      "train loss:1.0857336146295653\n",
      "train loss:1.0232369935286223\n",
      "train loss:1.098823561275749\n",
      "train loss:1.1101963003385589\n",
      "train loss:1.1323990327815983\n",
      "train loss:1.099341810973235\n",
      "train loss:1.1328307048150155\n",
      "train loss:0.9353654166497237\n",
      "train loss:0.9574647875950666\n",
      "train loss:1.0710201124599161\n",
      "train loss:1.140179012547468\n",
      "train loss:0.9828203750579108\n",
      "train loss:1.014629449664757\n",
      "train loss:1.2644261977908255\n",
      "train loss:1.002778028070251\n",
      "train loss:1.017760310133189\n",
      "train loss:1.0425512408941167\n",
      "train loss:0.9584337929058567\n",
      "train loss:1.083817783217556\n",
      "train loss:1.102432216693703\n",
      "train loss:1.0956432434894492\n",
      "train loss:1.1327649832540874\n",
      "train loss:1.2380324567583423\n",
      "train loss:1.0424531165193787\n",
      "train loss:0.9875924058881144\n",
      "train loss:1.2790367079919347\n",
      "train loss:1.1289826543334769\n",
      "train loss:1.0288994765667938\n",
      "train loss:0.9992132944809342\n",
      "train loss:1.0498351978049345\n",
      "train loss:0.9157582343756634\n",
      "train loss:1.0405038897434458\n",
      "train loss:0.9646754301007574\n",
      "train loss:1.0950428406128627\n",
      "train loss:0.9219393756354226\n",
      "train loss:1.016029877949087\n",
      "train loss:1.0295517931718863\n",
      "train loss:0.978180476478576\n",
      "train loss:1.0871543223738183\n",
      "train loss:1.1072012615912938\n",
      "train loss:0.8906861069231341\n",
      "train loss:1.2541284108251727\n",
      "train loss:0.9007703957431095\n",
      "train loss:0.9326876396789352\n",
      "train loss:0.9554965599961415\n",
      "train loss:1.2370683162639196\n",
      "train loss:0.8757122686749301\n",
      "train loss:0.8761944544945013\n",
      "train loss:0.978047334026802\n",
      "train loss:1.0937105358631556\n",
      "train loss:0.7669321709892227\n",
      "train loss:1.1280291723364213\n",
      "train loss:1.0252306371444824\n",
      "train loss:0.9218167845249994\n",
      "train loss:1.0432844869101818\n",
      "train loss:1.0979264871026941\n",
      "train loss:1.077513114257065\n",
      "train loss:1.2286585885377135\n",
      "train loss:0.824903900719148\n",
      "train loss:1.0595818554244993\n",
      "train loss:0.9843098756476562\n",
      "train loss:1.111950455204102\n",
      "train loss:1.1655788618217855\n",
      "train loss:0.9976691101656757\n",
      "train loss:1.1642821427155163\n",
      "train loss:1.167416739055742\n",
      "train loss:0.8570947963499959\n",
      "train loss:1.055169287395017\n",
      "train loss:1.0741881740299453\n",
      "train loss:0.902871608198303\n",
      "train loss:0.9624078223678287\n",
      "train loss:1.1241857256171996\n",
      "train loss:0.9390331810681445\n",
      "train loss:0.8657296976820413\n",
      "train loss:0.9649457389274726\n",
      "train loss:1.0125009612252551\n",
      "train loss:1.0088032583300262\n",
      "train loss:0.9276608156891768\n",
      "train loss:1.2090585927900763\n",
      "train loss:1.0458089697357436\n",
      "train loss:0.8500198129818226\n",
      "train loss:0.9222436950647264\n",
      "train loss:0.936048003554441\n",
      "train loss:0.9716695787606543\n",
      "train loss:1.139115375744445\n",
      "train loss:0.9654425408133013\n",
      "train loss:1.047822834727193\n",
      "train loss:0.926727313440191\n",
      "train loss:1.0558392496192972\n",
      "train loss:1.0301864134847083\n",
      "train loss:1.0188710456447587\n",
      "train loss:1.0914042039451153\n",
      "train loss:1.0923898150625369\n",
      "train loss:0.9486344017642847\n",
      "train loss:1.1804471121597973\n",
      "train loss:1.0140149127019717\n",
      "train loss:0.9956523558044175\n",
      "train loss:1.0604132782705675\n",
      "train loss:1.0302106494611614\n",
      "train loss:0.9072512359064399\n",
      "train loss:0.9067919503444805\n",
      "train loss:0.8469381784077101\n",
      "train loss:1.026602188437311\n",
      "train loss:1.0778570865438568\n",
      "train loss:1.1273067029916843\n",
      "train loss:1.0273839175625656\n",
      "train loss:1.0294876604362182\n",
      "train loss:1.0769402619275115\n",
      "train loss:1.1364992208051383\n",
      "train loss:1.0167597497104572\n",
      "train loss:0.9652245536572082\n",
      "train loss:1.0282844753547302\n",
      "train loss:0.9309289221118989\n",
      "train loss:1.002812894611872\n",
      "train loss:0.9785420070255856\n",
      "train loss:0.8900522533629085\n",
      "train loss:0.8938294504841069\n",
      "train loss:0.7341266353305476\n",
      "train loss:0.9348938396786711\n",
      "train loss:0.9989400198474594\n",
      "train loss:1.008282443234863\n",
      "train loss:0.9722423122262899\n",
      "train loss:0.8701759134786602\n",
      "train loss:0.9308176133537372\n",
      "train loss:1.0359495928800453\n",
      "train loss:1.002762750312713\n",
      "train loss:1.0222093341378011\n",
      "train loss:1.0906775352901728\n",
      "train loss:0.8853450524660951\n",
      "train loss:0.8296358497355416\n",
      "train loss:1.0238721384664466\n",
      "train loss:0.9347967151397043\n",
      "train loss:1.0761043584312124\n",
      "train loss:1.0901245591917657\n",
      "train loss:1.0736348444337485\n",
      "train loss:1.0086241306067594\n",
      "train loss:0.8893703325286428\n",
      "train loss:0.9223621176847085\n",
      "train loss:0.9356357159941969\n",
      "train loss:1.1718352299860433\n",
      "train loss:1.0821036955577654\n",
      "train loss:1.0563871318686398\n",
      "train loss:1.1039975741028256\n",
      "train loss:1.077580641146772\n",
      "train loss:0.9614319640732024\n",
      "train loss:1.0610393954930961\n",
      "train loss:0.895973395793535\n",
      "train loss:1.1707829067187285\n",
      "train loss:1.0639027756424182\n",
      "train loss:1.0737223396130622\n",
      "train loss:0.9541015659994702\n",
      "train loss:0.8945347501906663\n",
      "train loss:1.0675918170956866\n",
      "train loss:1.0791176601341363\n",
      "train loss:1.1128845025597687\n",
      "train loss:1.045063912072364\n",
      "train loss:0.8391810144238154\n",
      "train loss:1.167661859008688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.9433994796339487\n",
      "train loss:0.929538621386734\n",
      "train loss:0.9246299878053736\n",
      "train loss:0.861291796184491\n",
      "train loss:1.2025926505417797\n",
      "train loss:0.8992998322210416\n",
      "train loss:0.9645128588479708\n",
      "train loss:1.052601116331988\n",
      "train loss:1.0156287338034449\n",
      "train loss:0.9349301616401804\n",
      "train loss:1.066997382000654\n",
      "train loss:1.15371849563972\n",
      "train loss:0.9107774008192702\n",
      "train loss:0.9921302867096726\n",
      "train loss:0.9817909862453733\n",
      "train loss:1.161824728386125\n",
      "train loss:1.0422434707348556\n",
      "train loss:1.0217389790696239\n",
      "train loss:1.093767664498053\n",
      "train loss:1.0221270221708467\n",
      "train loss:1.1494419063893848\n",
      "train loss:1.1622132156220968\n",
      "train loss:0.992959100019393\n",
      "train loss:0.9350211542065633\n",
      "train loss:1.020590311632104\n",
      "train loss:1.0484238272155777\n",
      "train loss:1.1165464189333874\n",
      "train loss:1.129713312413613\n",
      "train loss:0.9278426784989406\n",
      "train loss:0.9968476633267427\n",
      "train loss:0.8445133793579517\n",
      "train loss:0.9269080474920615\n",
      "train loss:1.0430893991339005\n",
      "train loss:1.3534770212882465\n",
      "train loss:1.0960030804449903\n",
      "train loss:1.1936178730962033\n",
      "train loss:1.0004157417475736\n",
      "train loss:1.0364047210226885\n",
      "train loss:0.9187757521809427\n",
      "train loss:1.0140770971351882\n",
      "train loss:1.0594974671340955\n",
      "train loss:1.0960194027104986\n",
      "train loss:1.1343818609033705\n",
      "train loss:0.9370584464293964\n",
      "train loss:0.9655588495776289\n",
      "train loss:0.9737024469656042\n",
      "train loss:1.0809926318308447\n",
      "train loss:0.9322322836714407\n",
      "train loss:1.0029182111874446\n",
      "train loss:0.8479470429712154\n",
      "train loss:0.9564309603238718\n",
      "train loss:1.0862637512064568\n",
      "train loss:1.188149082532616\n",
      "train loss:0.9831242530654177\n",
      "train loss:0.9543032980950281\n",
      "train loss:0.9551068349432303\n",
      "train loss:0.9350089045109035\n",
      "train loss:0.980105293325554\n",
      "train loss:1.3086784372251368\n",
      "train loss:1.12263742978451\n",
      "train loss:0.8721786317978029\n",
      "train loss:0.9399536467520727\n",
      "train loss:0.9246732006968321\n",
      "train loss:0.8603987853874103\n",
      "train loss:0.9906492147619156\n",
      "train loss:1.0744021958999164\n",
      "train loss:0.8972945785040024\n",
      "train loss:1.0387996878440793\n",
      "train loss:1.1405474919885246\n",
      "train loss:1.0524530203319669\n",
      "train loss:1.1124409932920303\n",
      "train loss:1.066649893062653\n",
      "train loss:1.0589168591501419\n",
      "train loss:0.9279049903234368\n",
      "train loss:1.0636117128940468\n",
      "train loss:0.8805858832837518\n",
      "train loss:0.9168975705957724\n",
      "train loss:1.017519849439018\n",
      "train loss:0.9009622619246233\n",
      "train loss:0.9528619757542713\n",
      "train loss:0.8077739072140179\n",
      "train loss:1.0861891769238634\n",
      "train loss:1.0491701514789566\n",
      "train loss:0.9633760529790654\n",
      "train loss:0.8554479412553166\n",
      "train loss:0.9120071314998949\n",
      "train loss:0.8728889373550291\n",
      "train loss:0.9794412707256768\n",
      "train loss:0.9124933458033287\n",
      "train loss:0.9469440125057114\n",
      "train loss:1.1441649086200258\n",
      "train loss:1.0152723787340563\n",
      "train loss:1.0307572298768555\n",
      "train loss:1.199475954137822\n",
      "train loss:0.9998309781859609\n",
      "train loss:1.0558047331228508\n",
      "train loss:1.0245475203534935\n",
      "train loss:0.9564459737065794\n",
      "train loss:0.871525430545316\n",
      "train loss:0.9193230734665465\n",
      "train loss:1.1014371917029948\n",
      "train loss:0.8255997366127822\n",
      "train loss:1.0398882794616942\n",
      "train loss:1.0238507703893815\n",
      "train loss:1.0504797960629164\n",
      "train loss:1.0199997803175516\n",
      "train loss:1.13276697574396\n",
      "train loss:1.0539200995568148\n",
      "train loss:1.086939020622674\n",
      "train loss:1.0425361868731517\n",
      "train loss:1.0225015104525224\n",
      "train loss:0.9209788491395335\n",
      "train loss:1.0790173889791963\n",
      "train loss:0.8613309359505601\n",
      "train loss:0.9859783186384837\n",
      "train loss:0.9001309278388827\n",
      "train loss:0.8984575727864105\n",
      "train loss:1.0273753034830073\n",
      "train loss:0.8497617771589161\n",
      "train loss:0.9882257939270431\n",
      "train loss:1.0261671248036377\n",
      "train loss:1.1144799094353262\n",
      "train loss:1.124247056455669\n",
      "train loss:0.9509338319731222\n",
      "train loss:0.905276749640814\n",
      "train loss:1.0198955771424953\n",
      "train loss:0.9756895388395727\n",
      "train loss:0.958752476484558\n",
      "train loss:1.0625727940160057\n",
      "train loss:1.063027510350854\n",
      "train loss:1.0436422312555766\n",
      "train loss:0.9135988867372293\n",
      "train loss:0.8945416270964515\n",
      "train loss:0.8208556744236049\n",
      "train loss:1.1736380870414116\n",
      "train loss:0.9440530322005626\n",
      "train loss:0.9684794834326756\n",
      "train loss:1.118813051861204\n",
      "train loss:1.0109449084737043\n",
      "train loss:0.9726105913571409\n",
      "train loss:0.926156008176916\n",
      "train loss:1.0447936218222766\n",
      "train loss:1.0030338126839398\n",
      "train loss:1.0563301966060081\n",
      "train loss:0.9598384953109327\n",
      "train loss:1.055224921478371\n",
      "train loss:1.0729047530643623\n",
      "train loss:0.8998563738974893\n",
      "train loss:1.1899926278349018\n",
      "train loss:1.0366901368016679\n",
      "train loss:1.1229796531815137\n",
      "train loss:0.9088502930426583\n",
      "train loss:0.9506056080945574\n",
      "train loss:0.9656614932318172\n",
      "train loss:0.9694763404824002\n",
      "train loss:0.8741455358921542\n",
      "train loss:1.045295340397853\n",
      "train loss:1.0199390040532874\n",
      "train loss:0.9221487240105352\n",
      "train loss:1.0175540984607183\n",
      "train loss:1.0482158603650578\n",
      "train loss:0.9808167040719001\n",
      "train loss:0.9783963813129785\n",
      "train loss:0.9351661025074537\n",
      "train loss:0.8867238982429007\n",
      "train loss:0.9642636256755739\n",
      "train loss:1.110239396084472\n",
      "train loss:1.071237257590588\n",
      "train loss:1.0564115193062469\n",
      "train loss:0.9309125080668212\n",
      "train loss:0.9197627487759817\n",
      "train loss:1.1177677309080138\n",
      "train loss:1.1922866899390856\n",
      "train loss:1.0253524852756783\n",
      "train loss:1.0466530141671373\n",
      "train loss:1.0198434494010074\n",
      "train loss:0.8689478461910867\n",
      "train loss:0.9695352755543593\n",
      "train loss:1.0697647497401528\n",
      "train loss:1.033004321786717\n",
      "train loss:0.8072619288040888\n",
      "train loss:1.1690507208111802\n",
      "train loss:1.0582706398833202\n",
      "train loss:1.2432329562621443\n",
      "train loss:0.8873409872684452\n",
      "train loss:0.9091193476582117\n",
      "train loss:1.0347993634191193\n",
      "train loss:1.0140313630428452\n",
      "train loss:0.9744571439813317\n",
      "train loss:0.9617063637522011\n",
      "train loss:0.9508874476623987\n",
      "train loss:0.8978514814952612\n",
      "train loss:0.9949831486273719\n",
      "train loss:1.0613444619096577\n",
      "train loss:1.0114983005801994\n",
      "train loss:0.9866318455325533\n",
      "train loss:1.0203142120148647\n",
      "train loss:0.9720961494395337\n",
      "train loss:1.0268793445406574\n",
      "train loss:1.0144212730936883\n",
      "train loss:1.0824578515063772\n",
      "train loss:1.0814338626397135\n",
      "train loss:1.1328089438739803\n",
      "train loss:0.9197493698977641\n",
      "train loss:0.9887099201836977\n",
      "train loss:0.8736486950533496\n",
      "train loss:0.9993211867859224\n",
      "train loss:1.040422416561201\n",
      "train loss:1.0155036772210742\n",
      "train loss:0.9810616542654301\n",
      "train loss:1.0585529876246544\n",
      "train loss:0.9404204886304277\n",
      "train loss:0.9572842258479386\n",
      "train loss:1.0234373796994303\n",
      "train loss:1.2077003881493262\n",
      "train loss:0.9519839310930565\n",
      "train loss:0.8964123978758647\n",
      "train loss:1.02060542171031\n",
      "train loss:0.9822177001494482\n",
      "train loss:0.9581753835360355\n",
      "train loss:1.0643047348521526\n",
      "train loss:1.078361541685905\n",
      "train loss:1.0113067137750364\n",
      "train loss:1.185375036294959\n",
      "train loss:1.1351900887990583\n",
      "train loss:0.8167173945197238\n",
      "train loss:1.1253542613035146\n",
      "train loss:0.9095234558787864\n",
      "train loss:0.9680801531037311\n",
      "train loss:0.9654118428401387\n",
      "train loss:0.9733962587562263\n",
      "train loss:1.0548782880761327\n",
      "train loss:1.0287384287142982\n",
      "train loss:0.9819465537799816\n",
      "train loss:0.9328169466671566\n",
      "train loss:0.9197214319515077\n",
      "train loss:0.824586089640843\n",
      "train loss:1.1584108392227508\n",
      "train loss:1.0178902835517\n",
      "train loss:0.8240920439863081\n",
      "train loss:0.8394946680632549\n",
      "train loss:1.0687940073798161\n",
      "train loss:1.108059328231621\n",
      "train loss:1.027493571729294\n",
      "train loss:0.9948915858939412\n",
      "train loss:1.060870787056047\n",
      "train loss:1.075781523566528\n",
      "train loss:1.018390187978265\n",
      "train loss:1.098959529526357\n",
      "train loss:1.0343950887360611\n",
      "train loss:0.9086321578918948\n",
      "train loss:1.0535359844782393\n",
      "train loss:1.143749793038209\n",
      "train loss:1.0560928585508118\n",
      "train loss:1.0423061899842352\n",
      "train loss:0.9064294506298122\n",
      "train loss:0.9617666771774822\n",
      "train loss:1.1054950226672782\n",
      "train loss:0.9595905121962948\n",
      "train loss:0.9633965189558255\n",
      "train loss:0.7551414555189752\n",
      "train loss:1.067569000377908\n",
      "train loss:0.89791697803703\n",
      "train loss:0.7965479159082334\n",
      "train loss:1.0758064206131963\n",
      "train loss:1.0056471593617093\n",
      "train loss:1.0058374392919502\n",
      "train loss:1.0855056294684144\n",
      "train loss:0.9979083213829187\n",
      "train loss:0.9762618126865037\n",
      "train loss:0.9277712015664457\n",
      "train loss:0.8124386155205418\n",
      "train loss:0.8856007074942679\n",
      "train loss:1.0593827650989267\n",
      "train loss:1.0061592174692555\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2f84e80bc551>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_param\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                   evaluate_sample_num_per_epoch=1000)\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# 매개변수 보관\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-learning-from-scratch-master\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-learning-from-scratch-master\\common\\trainer.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mt_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-learning-from-scratch-master\\ch08\\deep_convnet.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;31m# forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m# backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-learning-from-scratch-master\\ch08\\deep_convnet.py\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_flg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-learning-from-scratch-master\\ch08\\deep_convnet.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, train_flg)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_flg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-learning-from-scratch-master\\common\\layers.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    218\u001b[0m         \u001b[0mout_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mFW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim2col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m         \u001b[0mcol_W\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-learning-from-scratch-master\\common\\util.py\u001b[0m in \u001b[0;36mim2col\u001b[1;34m(input_data, filter_h, filter_w, stride, pad)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mout_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpad\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfilter_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mstride\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'constant'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_w\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_w\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wltjr\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\numpy\\lib\\arraypad.py\u001b[0m in \u001b[0;36mpad\u001b[1;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[0;32m   1244\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad_before\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_after\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbefore_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mafter_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1245\u001b[0m                 \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpad_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'constant_values'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1246\u001b[1;33m             \u001b[0mnewmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prepend_const\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_before\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbefore_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1247\u001b[0m             \u001b[0mnewmat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_append_const\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_after\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mafter_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wltjr\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\numpy\\lib\\arraypad.py\u001b[0m in \u001b[0;36m_prepend_const\u001b[1;34m(arr, pad_amt, val, axis)\u001b[0m\n\u001b[0;32m    131\u001b[0m     padshape = tuple(x if i != axis else pad_amt\n\u001b[0;32m    132\u001b[0m                      for (i, x) in enumerate(arr.shape))\n\u001b[1;32m--> 133\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_do_prepend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\wltjr\\anaconda3\\envs\\pr_tensorflow\\lib\\site-packages\\numpy\\lib\\arraypad.py\u001b[0m in \u001b[0;36m_do_prepend\u001b[1;34m(arr, pad_chunk, axis)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_do_prepend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpad_chunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     return np.concatenate(\n\u001b[1;32m---> 99\u001b[1;33m         (pad_chunk.astype(arr.dtype, copy=False), arr), axis=axis)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from deep_convnet import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보관\n",
    "network.save_params(\"deep_convnet_params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-academy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
