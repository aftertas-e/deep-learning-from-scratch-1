{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hawaiian-marketplace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.3001097024107326\n",
      "=== epoch:1, train acc:0.142, test acc:0.118 ===\n",
      "train loss:2.29759485647904\n",
      "train loss:2.293003607052743\n",
      "train loss:2.2912539110784436\n",
      "train loss:2.27931805037202\n",
      "train loss:2.2661373268128226\n",
      "train loss:2.250466562641051\n",
      "train loss:2.247403187197968\n",
      "train loss:2.2300486728002804\n",
      "train loss:2.1907146193559925\n",
      "train loss:2.1631421575076413\n",
      "train loss:2.1368664789283724\n",
      "train loss:2.096273879421106\n",
      "train loss:2.0863373669921783\n",
      "train loss:1.9354212112488818\n",
      "train loss:1.9387812415974306\n",
      "train loss:1.8674108694192395\n",
      "train loss:1.8428122251228052\n",
      "train loss:1.702313586556206\n",
      "train loss:1.647327306336363\n",
      "train loss:1.5018071427364357\n",
      "train loss:1.4927845713469075\n",
      "train loss:1.4153481002428814\n",
      "train loss:1.3145003797044705\n",
      "train loss:1.243754435855128\n",
      "train loss:1.1896821055483238\n",
      "train loss:1.0964868137373454\n",
      "train loss:0.9933683960968187\n",
      "train loss:1.059301310570833\n",
      "train loss:0.9150122152264123\n",
      "train loss:0.7478672339634389\n",
      "train loss:0.8006004409967697\n",
      "train loss:0.8062189198298944\n",
      "train loss:0.7043348828235113\n",
      "train loss:0.6336795027426182\n",
      "train loss:0.7920899378151635\n",
      "train loss:0.6789759570061037\n",
      "train loss:0.5694752194667421\n",
      "train loss:0.6730195198967991\n",
      "train loss:0.5747825765367414\n",
      "train loss:0.5060550392574399\n",
      "train loss:0.6651620267474441\n",
      "train loss:0.619382725090135\n",
      "train loss:0.5986833197525443\n",
      "train loss:0.5668035039060045\n",
      "train loss:0.5494288620583208\n",
      "train loss:0.42007473611864965\n",
      "train loss:0.6661044933441871\n",
      "train loss:0.5122065583994647\n",
      "train loss:0.46365910073612715\n",
      "train loss:0.459276810090979\n",
      "=== epoch:2, train acc:0.832, test acc:0.806 ===\n",
      "train loss:0.6438606369151956\n",
      "train loss:0.33693159601651596\n",
      "train loss:0.37600617524115554\n",
      "train loss:0.48663193779296215\n",
      "train loss:0.695604105159638\n",
      "train loss:0.36650565153060116\n",
      "train loss:0.37778145735046137\n",
      "train loss:0.4802662103527875\n",
      "train loss:0.42836330264431877\n",
      "train loss:0.6348818443500556\n",
      "train loss:0.4446705712126466\n",
      "train loss:0.5556546697171277\n",
      "train loss:0.5285697884783203\n",
      "train loss:0.5369871146642201\n",
      "train loss:0.5015896603862319\n",
      "train loss:0.2312375189842168\n",
      "train loss:0.48121521345733426\n",
      "train loss:0.4128234761905057\n",
      "train loss:0.36732092974015434\n",
      "train loss:0.3422941010003364\n",
      "train loss:0.4921864384545602\n",
      "train loss:0.27295338208290487\n",
      "train loss:0.5457197891390863\n",
      "train loss:0.3891788514082565\n",
      "train loss:0.37137081845725134\n",
      "train loss:0.5709000604953277\n",
      "train loss:0.34924734183002504\n",
      "train loss:0.2704540127115549\n",
      "train loss:0.3775911580320222\n",
      "train loss:0.3929637388569472\n",
      "train loss:0.26032773106748897\n",
      "train loss:0.5765315612826257\n",
      "train loss:0.30635988779000256\n",
      "train loss:0.27092093358616315\n",
      "train loss:0.38800842192836016\n",
      "train loss:0.3921161008588879\n",
      "train loss:0.3414102206779833\n",
      "train loss:0.3712599574509039\n",
      "train loss:0.24014561309017204\n",
      "train loss:0.2734706950119257\n",
      "train loss:0.3140479976278987\n",
      "train loss:0.24166817458825932\n",
      "train loss:0.3203608689391411\n",
      "train loss:0.3190584898263911\n",
      "train loss:0.47521209274864645\n",
      "train loss:0.2895112167184094\n",
      "train loss:0.42784447592065133\n",
      "train loss:0.4289095785238505\n",
      "train loss:0.2050849228218221\n",
      "train loss:0.32055621069211604\n",
      "=== epoch:3, train acc:0.879, test acc:0.864 ===\n",
      "train loss:0.2948698240975108\n",
      "train loss:0.3648367935568922\n",
      "train loss:0.41038461528912795\n",
      "train loss:0.2923409945008377\n",
      "train loss:0.4203595261388736\n",
      "train loss:0.28488912807292666\n",
      "train loss:0.29383956185162463\n",
      "train loss:0.3833593202258419\n",
      "train loss:0.3336710952668048\n",
      "train loss:0.33352783679859377\n",
      "train loss:0.3689113642507267\n",
      "train loss:0.3408916188555356\n",
      "train loss:0.41320198342481407\n",
      "train loss:0.27669359967821394\n",
      "train loss:0.23400145807996495\n",
      "train loss:0.24155248177737754\n",
      "train loss:0.22671919410249378\n",
      "train loss:0.4729611341856756\n",
      "train loss:0.4481192141541575\n",
      "train loss:0.2461562277678932\n",
      "train loss:0.33057937799721904\n",
      "train loss:0.26895559165263727\n",
      "train loss:0.2721482843285493\n",
      "train loss:0.33428338823259984\n",
      "train loss:0.4309291887707543\n",
      "train loss:0.3740754799324902\n",
      "train loss:0.25286337634053974\n",
      "train loss:0.254824979290249\n",
      "train loss:0.268583370855795\n",
      "train loss:0.31407904714158996\n",
      "train loss:0.3832867172694742\n",
      "train loss:0.31104101259701705\n",
      "train loss:0.24629306422018238\n",
      "train loss:0.30699853484983214\n",
      "train loss:0.24768378638846997\n",
      "train loss:0.37564035818287883\n",
      "train loss:0.22756266254549928\n",
      "train loss:0.25834585621804623\n",
      "train loss:0.23744932007607006\n",
      "train loss:0.29012383338449454\n",
      "train loss:0.3009421245387558\n",
      "train loss:0.1696778812322405\n",
      "train loss:0.24497369230604804\n",
      "train loss:0.2595641311702195\n",
      "train loss:0.2560559991057583\n",
      "train loss:0.2675712567672543\n",
      "train loss:0.32116407665116553\n",
      "train loss:0.1970337312479523\n",
      "train loss:0.2630087630175687\n",
      "train loss:0.271346413396996\n",
      "=== epoch:4, train acc:0.886, test acc:0.894 ===\n",
      "train loss:0.3883728678973851\n",
      "train loss:0.28048438420642474\n",
      "train loss:0.2692425620203707\n",
      "train loss:0.11957672553437154\n",
      "train loss:0.3693285164030685\n",
      "train loss:0.2364199369969671\n",
      "train loss:0.3031556170356969\n",
      "train loss:0.23931307767945437\n",
      "train loss:0.3154778365578509\n",
      "train loss:0.3727935497973652\n",
      "train loss:0.333543064006299\n",
      "train loss:0.3201370016149625\n",
      "train loss:0.21550166921310396\n",
      "train loss:0.21769518129965365\n",
      "train loss:0.18458707712811367\n",
      "train loss:0.3234464341147563\n",
      "train loss:0.2539021979333961\n",
      "train loss:0.11749855103413047\n",
      "train loss:0.228099447105148\n",
      "train loss:0.27019201855769004\n",
      "train loss:0.20145770686244296\n",
      "train loss:0.31807972500310233\n",
      "train loss:0.12350026996765051\n",
      "train loss:0.2606969516062889\n",
      "train loss:0.22613354520681594\n",
      "train loss:0.261647321974449\n",
      "train loss:0.2957112808380618\n",
      "train loss:0.26499400727795036\n",
      "train loss:0.295581781358101\n",
      "train loss:0.1971552275607356\n",
      "train loss:0.24203199653380889\n",
      "train loss:0.2042705063053257\n",
      "train loss:0.2690493355838999\n",
      "train loss:0.23144469872204457\n",
      "train loss:0.2039498830855747\n",
      "train loss:0.3502436213214082\n",
      "train loss:0.16511609627433796\n",
      "train loss:0.19162464956292044\n",
      "train loss:0.25949530118508973\n",
      "train loss:0.1414722035852833\n",
      "train loss:0.21066735925853142\n",
      "train loss:0.2889278104781247\n",
      "train loss:0.22189102019639353\n",
      "train loss:0.19634670766119716\n",
      "train loss:0.22367502907715975\n",
      "train loss:0.2906769267556312\n",
      "train loss:0.2438269222091236\n",
      "train loss:0.2335008014295146\n",
      "train loss:0.2707232501624813\n",
      "train loss:0.1884421730539372\n",
      "=== epoch:5, train acc:0.907, test acc:0.892 ===\n",
      "train loss:0.2459600511692771\n",
      "train loss:0.21513546968858438\n",
      "train loss:0.25438872565040693\n",
      "train loss:0.4321227075984035\n",
      "train loss:0.24325407938907362\n",
      "train loss:0.35005184384455196\n",
      "train loss:0.1409142848411484\n",
      "train loss:0.1587713105928121\n",
      "train loss:0.26286789399058913\n",
      "train loss:0.19339076595154836\n",
      "train loss:0.2383943856838256\n",
      "train loss:0.3091354134331102\n",
      "train loss:0.35026553500068536\n",
      "train loss:0.18934840696758712\n",
      "train loss:0.25410564625915444\n",
      "train loss:0.18666735865928247\n",
      "train loss:0.17538677916225434\n",
      "train loss:0.24972251450356872\n",
      "train loss:0.18931426371794408\n",
      "train loss:0.1223967965725479\n",
      "train loss:0.24115107384832649\n",
      "train loss:0.2657127311336627\n",
      "train loss:0.19394201454236332\n",
      "train loss:0.16317204136966623\n",
      "train loss:0.21921793634432657\n",
      "train loss:0.21041318628049854\n",
      "train loss:0.12265910266349427\n",
      "train loss:0.20133099565839369\n",
      "train loss:0.10992406341517888\n",
      "train loss:0.19328860935480563\n",
      "train loss:0.17332971292440075\n",
      "train loss:0.17919785297322302\n",
      "train loss:0.2379621641724584\n",
      "train loss:0.23841885352768763\n",
      "train loss:0.23879610111878036\n",
      "train loss:0.16201760738580512\n",
      "train loss:0.30117481274495156\n",
      "train loss:0.12782654356336443\n",
      "train loss:0.37636859609968126\n",
      "train loss:0.2866264896822235\n",
      "train loss:0.21241977534222867\n",
      "train loss:0.2450769220696419\n",
      "train loss:0.1181356474716789\n",
      "train loss:0.19968683290620615\n",
      "train loss:0.2204046474132895\n",
      "train loss:0.19810202137034758\n",
      "train loss:0.17415144608053662\n",
      "train loss:0.14768616016468333\n",
      "train loss:0.10930331298064033\n",
      "train loss:0.3330894934091351\n",
      "=== epoch:6, train acc:0.921, test acc:0.911 ===\n",
      "train loss:0.29065906451340345\n",
      "train loss:0.22474719328821224\n",
      "train loss:0.1536552117379737\n",
      "train loss:0.19082998417654093\n",
      "train loss:0.1599299382099905\n",
      "train loss:0.17560816856973818\n",
      "train loss:0.19938623265885386\n",
      "train loss:0.10272527946723356\n",
      "train loss:0.20640530362740855\n",
      "train loss:0.24034619945712066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.2736571032767485\n",
      "train loss:0.2549525160176456\n",
      "train loss:0.0885893929185136\n",
      "train loss:0.35006284548921246\n",
      "train loss:0.1230621595157492\n",
      "train loss:0.15290134123739385\n",
      "train loss:0.2111554739980495\n",
      "train loss:0.38949104636332094\n",
      "train loss:0.28164825895082995\n",
      "train loss:0.11806621408902963\n",
      "train loss:0.23722130842524705\n",
      "train loss:0.16767834497148473\n",
      "train loss:0.26749071201018876\n",
      "train loss:0.17585882284845014\n",
      "train loss:0.28737416966819196\n",
      "train loss:0.27692529603365906\n",
      "train loss:0.19023861621291938\n",
      "train loss:0.10197516912526743\n",
      "train loss:0.17351425040138202\n",
      "train loss:0.2535890822499404\n",
      "train loss:0.1812883987670223\n",
      "train loss:0.3043409055625979\n",
      "train loss:0.19682164235099373\n",
      "train loss:0.16483162678495425\n",
      "train loss:0.1507089390468702\n",
      "train loss:0.23100194982539532\n",
      "train loss:0.29082454264051305\n",
      "train loss:0.196927851255351\n",
      "train loss:0.12400818396937774\n",
      "train loss:0.11785783510682558\n",
      "train loss:0.15473297598684327\n",
      "train loss:0.12531185545378623\n",
      "train loss:0.20120866304605112\n",
      "train loss:0.13003938065048476\n",
      "train loss:0.1373968951988152\n",
      "train loss:0.1482115098719709\n",
      "train loss:0.24622861445347763\n",
      "train loss:0.08980672651139691\n",
      "train loss:0.2154262538055031\n",
      "train loss:0.16404259187208742\n",
      "=== epoch:7, train acc:0.938, test acc:0.924 ===\n",
      "train loss:0.25327272835381776\n",
      "train loss:0.111763513350878\n",
      "train loss:0.15492520553152592\n",
      "train loss:0.17841234177690954\n",
      "train loss:0.20040143442845046\n",
      "train loss:0.15200282648245916\n",
      "train loss:0.22791457653536049\n",
      "train loss:0.12919611427365868\n",
      "train loss:0.25374412989813444\n",
      "train loss:0.09842464394666647\n",
      "train loss:0.16913400706916326\n",
      "train loss:0.09767178664084705\n",
      "train loss:0.17092620786270968\n",
      "train loss:0.1885842752940121\n",
      "train loss:0.08939358521471809\n",
      "train loss:0.17192980219809598\n",
      "train loss:0.19059108479720377\n",
      "train loss:0.1391972928517682\n",
      "train loss:0.12711987570482072\n",
      "train loss:0.18761835746639113\n",
      "train loss:0.14266282259056307\n",
      "train loss:0.14083980246521433\n",
      "train loss:0.28414423341070416\n",
      "train loss:0.11910337043120471\n",
      "train loss:0.1807040539623153\n",
      "train loss:0.17414703973930964\n",
      "train loss:0.10411398491917641\n",
      "train loss:0.16179668411282275\n",
      "train loss:0.15602033151348604\n",
      "train loss:0.09692068655469241\n",
      "train loss:0.16868159968221483\n",
      "train loss:0.1008471971796013\n",
      "train loss:0.0941931940143073\n",
      "train loss:0.07235751849097136\n",
      "train loss:0.11247413185923355\n",
      "train loss:0.1540170665615464\n",
      "train loss:0.2515903885045743\n",
      "train loss:0.13547189076740568\n",
      "train loss:0.11668169010938147\n",
      "train loss:0.12216856862195076\n",
      "train loss:0.11118732112247015\n",
      "train loss:0.19049604435137088\n",
      "train loss:0.1446998823796153\n",
      "train loss:0.10405624332958993\n",
      "train loss:0.09349685506061642\n",
      "train loss:0.12587451243199685\n",
      "train loss:0.10052076232864474\n",
      "train loss:0.09783266566467001\n",
      "train loss:0.12040346133383269\n",
      "train loss:0.12712769523165657\n",
      "=== epoch:8, train acc:0.947, test acc:0.929 ===\n",
      "train loss:0.11877241899447051\n",
      "train loss:0.11340422854023294\n",
      "train loss:0.1744483426372408\n",
      "train loss:0.10653845108531852\n",
      "train loss:0.11491836793507786\n",
      "train loss:0.10958729934600481\n",
      "train loss:0.1765925614907862\n",
      "train loss:0.10250350900318285\n",
      "train loss:0.10648699243198466\n",
      "train loss:0.2721238935810931\n",
      "train loss:0.15209781772599007\n",
      "train loss:0.12409990143071573\n",
      "train loss:0.15495048447828294\n",
      "train loss:0.15803584094864054\n",
      "train loss:0.0694075260860377\n",
      "train loss:0.11078606129530191\n",
      "train loss:0.13290726104214337\n",
      "train loss:0.05725616019091692\n",
      "train loss:0.15343990035160357\n",
      "train loss:0.18919032430501442\n",
      "train loss:0.1528697216951336\n",
      "train loss:0.08067681939411628\n",
      "train loss:0.15788769224277852\n",
      "train loss:0.0749823370054445\n",
      "train loss:0.10643248342583232\n",
      "train loss:0.06651070421600747\n",
      "train loss:0.08066378573776918\n",
      "train loss:0.13861816579468095\n",
      "train loss:0.0957287389057738\n",
      "train loss:0.12745106911909437\n",
      "train loss:0.12865159734377415\n",
      "train loss:0.09546991607037265\n",
      "train loss:0.08570103878610612\n",
      "train loss:0.15153647427339867\n",
      "train loss:0.18173683287009365\n",
      "train loss:0.12758087579954835\n",
      "train loss:0.14911387786130553\n",
      "train loss:0.07995750379695882\n",
      "train loss:0.1376545294556624\n",
      "train loss:0.1550993158742827\n",
      "train loss:0.08805528361038094\n",
      "train loss:0.08362242993322852\n",
      "train loss:0.13446874937610231\n",
      "train loss:0.11496791007730257\n",
      "train loss:0.11845452316926139\n",
      "train loss:0.09163990931477059\n",
      "train loss:0.15571768073801606\n",
      "train loss:0.10797308659351983\n",
      "train loss:0.11329541513522023\n",
      "train loss:0.111989894779476\n",
      "=== epoch:9, train acc:0.958, test acc:0.938 ===\n",
      "train loss:0.0985930858781626\n",
      "train loss:0.051328020576566005\n",
      "train loss:0.08001607434556321\n",
      "train loss:0.09334833095813976\n",
      "train loss:0.1695253562900914\n",
      "train loss:0.13485107619678352\n",
      "train loss:0.1970555882781241\n",
      "train loss:0.1446326540005538\n",
      "train loss:0.11508266359551621\n",
      "train loss:0.08155732580734827\n",
      "train loss:0.07014846852232906\n",
      "train loss:0.12922730491136028\n",
      "train loss:0.14118496412514436\n",
      "train loss:0.10752074801784162\n",
      "train loss:0.24911307076392616\n",
      "train loss:0.20683884527336382\n",
      "train loss:0.07423261303122887\n",
      "train loss:0.089579308326634\n",
      "train loss:0.14034099844685247\n",
      "train loss:0.08346339735253946\n",
      "train loss:0.09793975890674027\n",
      "train loss:0.11539674539671164\n",
      "train loss:0.11942201965041557\n",
      "train loss:0.17483598446822668\n",
      "train loss:0.11117479834759886\n",
      "train loss:0.09736949700107049\n",
      "train loss:0.05987887577016519\n",
      "train loss:0.07922752930794906\n",
      "train loss:0.054389401137506954\n",
      "train loss:0.12826292048929822\n",
      "train loss:0.16356116333213813\n",
      "train loss:0.07967424042954785\n",
      "train loss:0.06133686616675463\n",
      "train loss:0.07423956964303938\n",
      "train loss:0.17119010030520015\n",
      "train loss:0.07566227247958014\n",
      "train loss:0.06455734616528074\n",
      "train loss:0.07967872163514737\n",
      "train loss:0.10639526748942624\n",
      "train loss:0.08559188404674298\n",
      "train loss:0.08922130909150622\n",
      "train loss:0.11085619152889993\n",
      "train loss:0.07810576520263682\n",
      "train loss:0.10226139213339083\n",
      "train loss:0.09693757946428358\n",
      "train loss:0.1561317234338834\n",
      "train loss:0.1768758205576715\n",
      "train loss:0.09326108711340232\n",
      "train loss:0.05907127684422519\n",
      "train loss:0.1358922530228867\n",
      "=== epoch:10, train acc:0.956, test acc:0.944 ===\n",
      "train loss:0.17611887839071272\n",
      "train loss:0.06316759223746074\n",
      "train loss:0.03474127195809613\n",
      "train loss:0.11000484021062652\n",
      "train loss:0.12070436953132838\n",
      "train loss:0.06273885958520538\n",
      "train loss:0.13225152881816205\n",
      "train loss:0.09208134312146826\n",
      "train loss:0.07836505967943323\n",
      "train loss:0.07692074032013892\n",
      "train loss:0.08065160308186015\n",
      "train loss:0.18005679827477067\n",
      "train loss:0.04792398140106568\n",
      "train loss:0.08190601419190639\n",
      "train loss:0.05587266317761983\n",
      "train loss:0.10847170091387844\n",
      "train loss:0.058115990835662684\n",
      "train loss:0.06362278013034356\n",
      "train loss:0.10885747911205318\n",
      "train loss:0.06516390821556435\n",
      "train loss:0.052633349333627205\n",
      "train loss:0.1291889628945864\n",
      "train loss:0.07090718781870614\n",
      "train loss:0.05610235693797225\n",
      "train loss:0.08355629200185812\n",
      "train loss:0.14205031279166078\n",
      "train loss:0.09248236092116127\n",
      "train loss:0.07980908626159336\n",
      "train loss:0.11488601589019451\n",
      "train loss:0.10630344112324351\n",
      "train loss:0.11902353958189249\n",
      "train loss:0.1252818788506653\n",
      "train loss:0.09839385278689176\n",
      "train loss:0.03879088491287206\n",
      "train loss:0.06755063412178525\n",
      "train loss:0.08531743785667908\n",
      "train loss:0.13186216774103832\n",
      "train loss:0.24441991920500658\n",
      "train loss:0.0417223113526018\n",
      "train loss:0.14437368004071963\n",
      "train loss:0.10145702292766128\n",
      "train loss:0.06065740849464451\n",
      "train loss:0.06147982938608063\n",
      "train loss:0.0780733007889522\n",
      "train loss:0.06840253239468944\n",
      "train loss:0.1159064490469304\n",
      "train loss:0.09363379867632454\n",
      "train loss:0.08684857875045474\n",
      "train loss:0.09374783848345299\n",
      "train loss:0.06290208188372085\n",
      "=== epoch:11, train acc:0.965, test acc:0.949 ===\n",
      "train loss:0.05924139886407476\n",
      "train loss:0.13694982765654268\n",
      "train loss:0.05973351968333906\n",
      "train loss:0.07433389254881889\n",
      "train loss:0.10000220158876534\n",
      "train loss:0.1220786664058144\n",
      "train loss:0.04768103657627457\n",
      "train loss:0.10847795748383078\n",
      "train loss:0.04034601992879546\n",
      "train loss:0.056826007858098146\n",
      "train loss:0.04062156962204142\n",
      "train loss:0.06306521734945167\n",
      "train loss:0.07179435457868887\n",
      "train loss:0.07174769706483708\n",
      "train loss:0.11678399170773153\n",
      "train loss:0.056706415287484796\n",
      "train loss:0.0334906729034669\n",
      "train loss:0.10647126510599383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.06535941145576243\n",
      "train loss:0.042623171023762556\n",
      "train loss:0.21355476858551378\n",
      "train loss:0.05184232888079912\n",
      "train loss:0.04915084167289345\n",
      "train loss:0.06395951244555534\n",
      "train loss:0.057020797102874804\n",
      "train loss:0.1529218134195873\n",
      "train loss:0.07719311297391901\n",
      "train loss:0.10877883681137736\n",
      "train loss:0.05216880807247027\n",
      "train loss:0.05134969110722319\n",
      "train loss:0.04246677600817478\n",
      "train loss:0.06060952506891016\n",
      "train loss:0.19233420372639828\n",
      "train loss:0.05652550810899343\n",
      "train loss:0.0426929869775283\n",
      "train loss:0.08183990612491122\n",
      "train loss:0.09531825591242367\n",
      "train loss:0.0398302081865294\n",
      "train loss:0.09676496163467557\n",
      "train loss:0.05802140926772858\n",
      "train loss:0.06906380005123734\n",
      "train loss:0.027374730726158578\n",
      "train loss:0.08379613504301635\n",
      "train loss:0.028371628396489948\n",
      "train loss:0.04600645630199587\n",
      "train loss:0.08906382615439645\n",
      "train loss:0.11095184136791306\n",
      "train loss:0.09915467999566877\n",
      "train loss:0.08290597981735366\n",
      "train loss:0.0723929348867376\n",
      "=== epoch:12, train acc:0.968, test acc:0.96 ===\n",
      "train loss:0.06696418866928394\n",
      "train loss:0.1721731415702502\n",
      "train loss:0.04956598257897818\n",
      "train loss:0.02830905254360925\n",
      "train loss:0.05970300346619979\n",
      "train loss:0.09356627280611392\n",
      "train loss:0.07820323787276812\n",
      "train loss:0.08931287544709172\n",
      "train loss:0.08528573292967483\n",
      "train loss:0.06816388319879965\n",
      "train loss:0.11970078742586189\n",
      "train loss:0.054532921414035915\n",
      "train loss:0.10723114240059259\n",
      "train loss:0.06976098675988757\n",
      "train loss:0.04238691801788409\n",
      "train loss:0.05421669566196658\n",
      "train loss:0.0662797121325111\n",
      "train loss:0.05434915227274992\n",
      "train loss:0.10831533101301609\n",
      "train loss:0.039908956033568685\n",
      "train loss:0.06923578500116462\n",
      "train loss:0.04982010123403071\n",
      "train loss:0.018189624259169077\n",
      "train loss:0.06763375789678623\n",
      "train loss:0.08120911277313052\n",
      "train loss:0.017260139558282213\n",
      "train loss:0.04155053159334826\n",
      "train loss:0.08789631490468489\n",
      "train loss:0.061394009166284\n",
      "train loss:0.07489582686413813\n",
      "train loss:0.04118713471621639\n",
      "train loss:0.09412957448945958\n",
      "train loss:0.07748379392463961\n",
      "train loss:0.027782269550791553\n",
      "train loss:0.06451985106914061\n",
      "train loss:0.1639132184829163\n",
      "train loss:0.05726680847498144\n",
      "train loss:0.05618057233375734\n",
      "train loss:0.05371644280964306\n",
      "train loss:0.1377127949804771\n",
      "train loss:0.08725133916927008\n",
      "train loss:0.057355761778560034\n",
      "train loss:0.04508029629435098\n",
      "train loss:0.041664256897748525\n",
      "train loss:0.11208097987856881\n",
      "train loss:0.03682138933288316\n",
      "train loss:0.023468938378843975\n",
      "train loss:0.03744351664389274\n",
      "train loss:0.057785409660998995\n",
      "train loss:0.032466653695358\n",
      "=== epoch:13, train acc:0.973, test acc:0.954 ===\n",
      "train loss:0.04656347096958258\n",
      "train loss:0.036248140284016635\n",
      "train loss:0.0909788154160296\n",
      "train loss:0.05407627548004327\n",
      "train loss:0.029833327142873633\n",
      "train loss:0.09189041274490303\n",
      "train loss:0.04681724048214937\n",
      "train loss:0.04699690207351964\n",
      "train loss:0.03539319012245849\n",
      "train loss:0.017567820328236994\n",
      "train loss:0.04528706088159898\n",
      "train loss:0.055375597671436676\n",
      "train loss:0.020149549745108762\n",
      "train loss:0.1566969269587445\n",
      "train loss:0.1186326456169719\n",
      "train loss:0.047366497995903256\n",
      "train loss:0.03650578030803383\n",
      "train loss:0.09487262587256179\n",
      "train loss:0.056434029710437913\n",
      "train loss:0.0798064870570729\n",
      "train loss:0.07064938032398943\n",
      "train loss:0.1400056252926472\n",
      "train loss:0.02330177848162594\n",
      "train loss:0.062154630538934906\n",
      "train loss:0.042781555142912465\n",
      "train loss:0.04455211670617025\n",
      "train loss:0.01887527527337016\n",
      "train loss:0.11652106629305788\n",
      "train loss:0.03574362478661098\n",
      "train loss:0.11812462154483155\n",
      "train loss:0.05978768698459714\n",
      "train loss:0.04672086272355891\n",
      "train loss:0.08696044975382289\n",
      "train loss:0.04498046355994312\n",
      "train loss:0.034479557273720596\n",
      "train loss:0.06431223404959202\n",
      "train loss:0.06006417004678105\n",
      "train loss:0.06840772567089241\n",
      "train loss:0.097124691912698\n",
      "train loss:0.08752432098358347\n",
      "train loss:0.04367779514099105\n",
      "train loss:0.04268795308151371\n",
      "train loss:0.1646160999434443\n",
      "train loss:0.03640564406061911\n",
      "train loss:0.04398311231438364\n",
      "train loss:0.0529420705942136\n",
      "train loss:0.0473696149036138\n",
      "train loss:0.05514599385338567\n",
      "train loss:0.09447326609196052\n",
      "train loss:0.07246731012557615\n",
      "=== epoch:14, train acc:0.979, test acc:0.956 ===\n",
      "train loss:0.06494683913297516\n",
      "train loss:0.021960332801887642\n",
      "train loss:0.14740841353800294\n",
      "train loss:0.02671999279824352\n",
      "train loss:0.028355978796208135\n",
      "train loss:0.03602824783104428\n",
      "train loss:0.059504192206877714\n",
      "train loss:0.04438018575708053\n",
      "train loss:0.040338250638273516\n",
      "train loss:0.04419169532522015\n",
      "train loss:0.050048525465506304\n",
      "train loss:0.10081372557408665\n",
      "train loss:0.022033053715536034\n",
      "train loss:0.01390075294298543\n",
      "train loss:0.03519309937424075\n",
      "train loss:0.03809329156981277\n",
      "train loss:0.05261788083814761\n",
      "train loss:0.023414360210512762\n",
      "train loss:0.10025753696396228\n",
      "train loss:0.08490628971367237\n",
      "train loss:0.08279757148501504\n",
      "train loss:0.057935206308825975\n",
      "train loss:0.08284836808664353\n",
      "train loss:0.0998061871016953\n",
      "train loss:0.03159466354843838\n",
      "train loss:0.05377051245839847\n",
      "train loss:0.08037577787891172\n",
      "train loss:0.059514694975128436\n",
      "train loss:0.04592161209193031\n",
      "train loss:0.07148046770893507\n",
      "train loss:0.06390765491903068\n",
      "train loss:0.07470169274597488\n",
      "train loss:0.05022636483481079\n",
      "train loss:0.014701927580825926\n",
      "train loss:0.02418159194351177\n",
      "train loss:0.05308670609795106\n",
      "train loss:0.07969135971031055\n",
      "train loss:0.06142753580568882\n",
      "train loss:0.016392249797374685\n",
      "train loss:0.016877990921012866\n",
      "train loss:0.019058446173389074\n",
      "train loss:0.027165629474671635\n",
      "train loss:0.03735022618476172\n",
      "train loss:0.08192586961222671\n",
      "train loss:0.06916702949814288\n",
      "train loss:0.02593974698519022\n",
      "train loss:0.030831418808635772\n",
      "train loss:0.046909546422071194\n",
      "train loss:0.045636046331083584\n",
      "train loss:0.07217373277044949\n",
      "=== epoch:15, train acc:0.98, test acc:0.956 ===\n",
      "train loss:0.037901853309528395\n",
      "train loss:0.02760441436740908\n",
      "train loss:0.01165158016701748\n",
      "train loss:0.027562794337932957\n",
      "train loss:0.0576047571343192\n",
      "train loss:0.028633345693600466\n",
      "train loss:0.049717237547705705\n",
      "train loss:0.040479457449994316\n",
      "train loss:0.05098991943065794\n",
      "train loss:0.05263161268956147\n",
      "train loss:0.0266762988952909\n",
      "train loss:0.020440126086631677\n",
      "train loss:0.03929134511452377\n",
      "train loss:0.02425372487018551\n",
      "train loss:0.04246631454899036\n",
      "train loss:0.04173763703196023\n",
      "train loss:0.032502261538423685\n",
      "train loss:0.026354463052097495\n",
      "train loss:0.029481872072954333\n",
      "train loss:0.02445952697377299\n",
      "train loss:0.024458897267238627\n",
      "train loss:0.035295624424270935\n",
      "train loss:0.03554426086736393\n",
      "train loss:0.11490358147301363\n",
      "train loss:0.018735873136044662\n",
      "train loss:0.027604837965778048\n",
      "train loss:0.026296777236861874\n",
      "train loss:0.016214584422895046\n",
      "train loss:0.12245456966287245\n",
      "train loss:0.04525511859112866\n",
      "train loss:0.1803438242832363\n",
      "train loss:0.04728600172901721\n",
      "train loss:0.014488215664707971\n",
      "train loss:0.05907556368022913\n",
      "train loss:0.04297738552816757\n",
      "train loss:0.035893870709645566\n",
      "train loss:0.06146664138105173\n",
      "train loss:0.020550629341095174\n",
      "train loss:0.029910399945511968\n",
      "train loss:0.027778254103630804\n",
      "train loss:0.09240200500857948\n",
      "train loss:0.04825568704549113\n",
      "train loss:0.03109826806631999\n",
      "train loss:0.05561609851036892\n",
      "train loss:0.018553416691206064\n",
      "train loss:0.017174431934472285\n",
      "train loss:0.01569689164756541\n",
      "train loss:0.028288153080505737\n",
      "train loss:0.017430862699886314\n",
      "train loss:0.028538636230189512\n",
      "=== epoch:16, train acc:0.981, test acc:0.955 ===\n",
      "train loss:0.05382254537151836\n",
      "train loss:0.032528949458136495\n",
      "train loss:0.06620935570317567\n",
      "train loss:0.020759261871146673\n",
      "train loss:0.032462800905757004\n",
      "train loss:0.04060380053531083\n",
      "train loss:0.03461662305084094\n",
      "train loss:0.057438576701964006\n",
      "train loss:0.03987274346705841\n",
      "train loss:0.019309884312963564\n",
      "train loss:0.04400277383533482\n",
      "train loss:0.015838713011196603\n",
      "train loss:0.01898823797068645\n",
      "train loss:0.02548970365134532\n",
      "train loss:0.018024476697134396\n",
      "train loss:0.027539740528686903\n",
      "train loss:0.027293655369594022\n",
      "train loss:0.06655081170808946\n",
      "train loss:0.010781312685596227\n",
      "train loss:0.012387525741085547\n",
      "train loss:0.025926922616238426\n",
      "train loss:0.023735410390409176\n",
      "train loss:0.028334496404406596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.03731994917382292\n",
      "train loss:0.0311973070290453\n",
      "train loss:0.03219645627228193\n",
      "train loss:0.012562181117784845\n",
      "train loss:0.03214327265586447\n",
      "train loss:0.04550684401568034\n",
      "train loss:0.07481248331725233\n",
      "train loss:0.021169296476145336\n",
      "train loss:0.02613605781727417\n",
      "train loss:0.020274544410215704\n",
      "train loss:0.03238531471869289\n",
      "train loss:0.03248257111477269\n",
      "train loss:0.0259318811193648\n",
      "train loss:0.05193815825991321\n",
      "train loss:0.026458992750555122\n",
      "train loss:0.020554177933992807\n",
      "train loss:0.023887143685945314\n",
      "train loss:0.020695908601833728\n",
      "train loss:0.02770885682024922\n",
      "train loss:0.03529646962318017\n",
      "train loss:0.020967844209630253\n",
      "train loss:0.027981021917619944\n",
      "train loss:0.031211387194057107\n",
      "train loss:0.03815904498983451\n",
      "train loss:0.016717376474746852\n",
      "train loss:0.02973716905432674\n",
      "train loss:0.015555568103431188\n",
      "=== epoch:17, train acc:0.987, test acc:0.962 ===\n",
      "train loss:0.018753253829275368\n",
      "train loss:0.02808978415962419\n",
      "train loss:0.01927642067087896\n",
      "train loss:0.011231751509757318\n",
      "train loss:0.020641948803227676\n",
      "train loss:0.035929487063488\n",
      "train loss:0.06079435897763006\n",
      "train loss:0.01026985584479847\n",
      "train loss:0.03176426698078994\n",
      "train loss:0.04710957054246324\n",
      "train loss:0.025337581654814357\n",
      "train loss:0.02126674336371043\n",
      "train loss:0.016943796713459244\n",
      "train loss:0.0372887129635042\n",
      "train loss:0.009025362536135874\n",
      "train loss:0.020950882656945443\n",
      "train loss:0.020412877134109872\n",
      "train loss:0.015405442735587346\n",
      "train loss:0.017517893298338437\n",
      "train loss:0.013256945577285049\n",
      "train loss:0.01675649551072889\n",
      "train loss:0.07473546921140115\n",
      "train loss:0.011029731449526053\n",
      "train loss:0.01171741604279818\n",
      "train loss:0.01720928704134748\n",
      "train loss:0.04160670671130081\n",
      "train loss:0.047325979621823826\n",
      "train loss:0.025358867062738395\n",
      "train loss:0.01007719269896012\n",
      "train loss:0.02006702610889583\n",
      "train loss:0.03755071244962366\n",
      "train loss:0.03517152304735009\n",
      "train loss:0.015049449232709962\n",
      "train loss:0.03183074861214264\n",
      "train loss:0.017750701469880645\n",
      "train loss:0.015541412617199523\n",
      "train loss:0.0279101645269506\n",
      "train loss:0.03392775786994466\n",
      "train loss:0.05779301338725906\n",
      "train loss:0.01386228158151053\n",
      "train loss:0.0244543555655611\n",
      "train loss:0.032893523153593035\n",
      "train loss:0.029190884544400074\n",
      "train loss:0.011638011699170758\n",
      "train loss:0.04167860164101291\n",
      "train loss:0.0073490488232240865\n",
      "train loss:0.011505425382906064\n",
      "train loss:0.012089989209364873\n",
      "train loss:0.02662247424688013\n",
      "train loss:0.031569865478211195\n",
      "=== epoch:18, train acc:0.985, test acc:0.96 ===\n",
      "train loss:0.025776437105652486\n",
      "train loss:0.014372021121927384\n",
      "train loss:0.012463128602192185\n",
      "train loss:0.029326865956525512\n",
      "train loss:0.030128975761640267\n",
      "train loss:0.025211391122015737\n",
      "train loss:0.016644328016078097\n",
      "train loss:0.008066678839818743\n",
      "train loss:0.02808777411312926\n",
      "train loss:0.039709503902860904\n",
      "train loss:0.01908927470864209\n",
      "train loss:0.012685646025685407\n",
      "train loss:0.01725476994726781\n",
      "train loss:0.017396134505749474\n",
      "train loss:0.013060222157658079\n",
      "train loss:0.01300798044039894\n",
      "train loss:0.027418611973300616\n",
      "train loss:0.022265523780579857\n",
      "train loss:0.025635895250822396\n",
      "train loss:0.028583576604866635\n",
      "train loss:0.06150499097692832\n",
      "train loss:0.015045111455014849\n",
      "train loss:0.034143792975307764\n",
      "train loss:0.01932477986938237\n",
      "train loss:0.040811276718058405\n",
      "train loss:0.015518855583697368\n",
      "train loss:0.004900376658878252\n",
      "train loss:0.014984182276864511\n",
      "train loss:0.01821714006154097\n",
      "train loss:0.035443174773276735\n",
      "train loss:0.024668284947019958\n",
      "train loss:0.039924284505591906\n",
      "train loss:0.02148271384057888\n",
      "train loss:0.015564209314562451\n",
      "train loss:0.0233341806632617\n",
      "train loss:0.019380358205705005\n",
      "train loss:0.012310458403723256\n",
      "train loss:0.010876638815264487\n",
      "train loss:0.03058818448108155\n",
      "train loss:0.07168688510805163\n",
      "train loss:0.014215429526359897\n",
      "train loss:0.017800315148433315\n",
      "train loss:0.011318875383346902\n",
      "train loss:0.011869891032640505\n",
      "train loss:0.022657937318438306\n",
      "train loss:0.022687129334687847\n",
      "train loss:0.010946443907996293\n",
      "train loss:0.00613099057601711\n",
      "train loss:0.018126049676031295\n",
      "train loss:0.04651502954640549\n",
      "=== epoch:19, train acc:0.991, test acc:0.961 ===\n",
      "train loss:0.01035326388929061\n",
      "train loss:0.007135264500610303\n",
      "train loss:0.016382533801257315\n",
      "train loss:0.08799578082907969\n",
      "train loss:0.03471699413181048\n",
      "train loss:0.025571001818873774\n",
      "train loss:0.034699048239109784\n",
      "train loss:0.018414524994541284\n",
      "train loss:0.02674524692317474\n",
      "train loss:0.027644461533711837\n",
      "train loss:0.006818989810611774\n",
      "train loss:0.022238986709555774\n",
      "train loss:0.015920810009998457\n",
      "train loss:0.03208569480454396\n",
      "train loss:0.028991877508522158\n",
      "train loss:0.018924286810034118\n",
      "train loss:0.01229493189547785\n",
      "train loss:0.015430255602971073\n",
      "train loss:0.019003867005114982\n",
      "train loss:0.011470217941343158\n",
      "train loss:0.02384448364539117\n",
      "train loss:0.025677213374101223\n",
      "train loss:0.07423002120506081\n",
      "train loss:0.028419556251158964\n",
      "train loss:0.031113346558186965\n",
      "train loss:0.019432774926039596\n",
      "train loss:0.04667093245504014\n",
      "train loss:0.029589413888734498\n",
      "train loss:0.008788109813475107\n",
      "train loss:0.08720557809503594\n",
      "train loss:0.01693400392222208\n",
      "train loss:0.013500386268259055\n",
      "train loss:0.011570172405438332\n",
      "train loss:0.00797191854623434\n",
      "train loss:0.09062984594801281\n",
      "train loss:0.012676278523469362\n",
      "train loss:0.03614559724373882\n",
      "train loss:0.016822838434066026\n",
      "train loss:0.030758444238718367\n",
      "train loss:0.011424339037230603\n",
      "train loss:0.012591465957651059\n",
      "train loss:0.015864691502831187\n",
      "train loss:0.03739865268943317\n",
      "train loss:0.0248056208966276\n",
      "train loss:0.007744504721359833\n",
      "train loss:0.018746008566583986\n",
      "train loss:0.013572452622571236\n",
      "train loss:0.04287616737549908\n",
      "train loss:0.03040869411155383\n",
      "train loss:0.013575850683630708\n",
      "=== epoch:20, train acc:0.994, test acc:0.957 ===\n",
      "train loss:0.038659943869509814\n",
      "train loss:0.03495977814747798\n",
      "train loss:0.04637892609325758\n",
      "train loss:0.06038999864224682\n",
      "train loss:0.02017213135753271\n",
      "train loss:0.010589108655493703\n",
      "train loss:0.024624860374649\n",
      "train loss:0.042923687272817486\n",
      "train loss:0.007937264871889877\n",
      "train loss:0.02023261947786353\n",
      "train loss:0.050033544753309844\n",
      "train loss:0.021513021335077584\n",
      "train loss:0.027292609814809857\n",
      "train loss:0.02157549046763855\n",
      "train loss:0.04771402333329975\n",
      "train loss:0.014200323561169721\n",
      "train loss:0.02043283485403007\n",
      "train loss:0.022016373598896327\n",
      "train loss:0.01796045511133955\n",
      "train loss:0.03294195977548236\n",
      "train loss:0.013333502352014148\n",
      "train loss:0.017825001392675024\n",
      "train loss:0.04748604300190772\n",
      "train loss:0.01275460673278604\n",
      "train loss:0.02182360397146634\n",
      "train loss:0.012625478600504347\n",
      "train loss:0.005035365840980561\n",
      "train loss:0.027275494417224672\n",
      "train loss:0.015519414538648989\n",
      "train loss:0.03640641593039839\n",
      "train loss:0.011322536681261135\n",
      "train loss:0.013871962458272064\n",
      "train loss:0.009669625683102477\n",
      "train loss:0.011947714039248564\n",
      "train loss:0.01615697458653848\n",
      "train loss:0.013597008186691191\n",
      "train loss:0.02885135447097729\n",
      "train loss:0.0106059987177888\n",
      "train loss:0.032072654086475\n",
      "train loss:0.008242706798640188\n",
      "train loss:0.00696303314794558\n",
      "train loss:0.0211956683148535\n",
      "train loss:0.017387090289474348\n",
      "train loss:0.015250199594276584\n",
      "train loss:0.008542557089394567\n",
      "train loss:0.018999041472572403\n",
      "train loss:0.019845530639440122\n",
      "train loss:0.01038725572226792\n",
      "train loss:0.02382299216088704\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.962\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-madness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
